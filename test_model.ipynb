{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-15 20:15:13.573240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1736968513.615790    4399 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1736968513.628326    4399 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-15 20:15:13.736987: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/home/user-labsiv/Desktop/mohamed/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "I0000 00:00:1736968516.275669    4399 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21868 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
            "All PyTorch model weights were used when initializing TFGPT2Model.\n",
            "\n",
            "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
            "I0000 00:00:1736968521.191338    4399 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated caption: AA airplane flying in the sky.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers\n",
        "from transformers import TFGPT2Model, GPT2Tokenizer\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ImageFeatureExtractor(layers.Layer):\n",
        "    def __init__(self, output_dim):\n",
        "        super(ImageFeatureExtractor, self).__init__()\n",
        "        base_model = tf.keras.applications.MobileNetV2(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(224, 224, 3)\n",
        "        )\n",
        "        base_model.trainable = False\n",
        "        self.cnn = base_model\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "        self.projection = layers.Dense(output_dim, activation='relu')\n",
        "\n",
        "    def call(self, images):\n",
        "        x = self.cnn(images)\n",
        "        x = self.global_pool(x)\n",
        "        return self.projection(x)\n",
        "\n",
        "class ProjectionLayer(layers.Layer):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(ProjectionLayer, self).__init__()\n",
        "        self.dense = layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, image_features):\n",
        "        return self.dense(image_features)\n",
        "\n",
        "class ImageCaptioningModel(Model):\n",
        "    def __init__(self, max_length=50, vocab_size=50257):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "        self.gpt2 = TFGPT2Model.from_pretrained('gpt2')\n",
        "        self.gpt2.trainable = False\n",
        "        self.image_encoder = ImageFeatureExtractor(\n",
        "            output_dim=self.gpt2.config.hidden_size\n",
        "        )\n",
        "        self.projection = ProjectionLayer(self.gpt2.config.hidden_size)\n",
        "        self.output_layer = layers.Dense(vocab_size, activation='softmax')\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        images, text_tokens = inputs\n",
        "        image_features = self.image_encoder(images)\n",
        "        projected_features = self.projection(image_features)\n",
        "        text_outputs = self.gpt2(text_tokens, return_dict=True)\n",
        "        text_features = text_outputs.last_hidden_state\n",
        "        expanded_features = tf.expand_dims(projected_features, axis=1)\n",
        "        expanded_features = tf.tile(\n",
        "            expanded_features,\n",
        "            [1, tf.shape(text_features)[1], 1]\n",
        "        )\n",
        "        combined_features = text_features + expanded_features\n",
        "        outputs = self.output_layer(combined_features)\n",
        "        return outputs\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"Load and preprocess a single image.\"\"\"\n",
        "    img = Image.open(image_path)\n",
        "    img = img.convert('RGB')\n",
        "    img = img.resize((224, 224))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
        "    return img_array\n",
        "\n",
        "def generate_caption(model, image_path, max_length=50):\n",
        "    \"\"\"Generate a caption for a single image.\"\"\"\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    img = preprocess_image(image_path)\n",
        "    img = tf.expand_dims(img, 0)\n",
        "    current_tokens = tf.convert_to_tensor([[tokenizer.bos_token_id]], dtype=tf.int32)\n",
        "    generated_caption = []\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        predictions = model((img, current_tokens))\n",
        "        next_token = tf.argmax(predictions[:, -1, :], axis=-1)\n",
        "        token_id = int(next_token.numpy()[0])\n",
        "        generated_caption.append(token_id)\n",
        "\n",
        "        if token_id == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        current_tokens = tf.convert_to_tensor([generated_caption], dtype=tf.int32)\n",
        "\n",
        "    caption = tokenizer.decode(generated_caption, skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Example usage\n",
        "def test_model(checkpoint_path, image_path):\n",
        "    \"\"\"Test the model on a single image.\"\"\"\n",
        "    # Initialize model\n",
        "    model = ImageCaptioningModel(max_length=50)\n",
        "    \n",
        "    # Create a dummy input to build the model\n",
        "    dummy_image = tf.zeros((1, 224, 224, 3))\n",
        "    dummy_tokens = tf.zeros((1, 1), dtype=tf.int32)\n",
        "    _ = model((dummy_image, dummy_tokens))\n",
        "    \n",
        "    # Load weights\n",
        "    model.load_weights(checkpoint_path)\n",
        "    \n",
        "    # Generate caption\n",
        "    caption = generate_caption(model, image_path)\n",
        "    return caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2Model.\n",
            "\n",
            "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated caption: AA traffic light on a city street.\n"
          ]
        }
      ],
      "source": [
        "# Replace these paths with your actual paths\n",
        "model_path = \"model/image_captioning_model_epoch_from_train10.weights.h5\"  # Update with your model path\n",
        "image_path = \"archive/coco2017/test2017/000000581823.jpg\"  # Update with your test image path\n",
        "\n",
        "caption = test_model(model_path, image_path)\n",
        "print(f\"Generated caption: {caption}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
