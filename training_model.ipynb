{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 10:23:49.226500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736673829.269872    3619 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736673829.282840    3619 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-12 10:23:49.393237: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU memory growth setting failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user-labsiv/Desktop/mohamed/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set parameteres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736673832.028444    3619 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22086 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "All PyTorch model weights were used when initializing TFGPT2Model.\n",
      "\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading COCO annotations...\n",
      "Loading annotations from archive/coco2017/annotations/captions_train2017.json\n",
      "Loaded annotations for 118287 images\n",
      "Loaded 118287 image captions.\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3709 [00:00<?, ?it/s]I0000 00:00:1736673838.481438    3619 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "Training: 100%|██████████| 3709/3709 [32:01<00:00,  1.98it/s, loss=0.844]2025-01-12 10:55:56.510306: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Training: 100%|██████████| 3709/3709 [32:01<00:00,  1.93it/s, loss=0.844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.0186\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3709/3709 [30:13<00:00,  2.11it/s, loss=0.738]2025-01-12 11:26:10.010363: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Training: 100%|██████████| 3709/3709 [30:13<00:00,  2.05it/s, loss=0.738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.7774\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3709/3709 [30:51<00:00,  2.00it/s, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.7327\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3709/3709 [31:02<00:00,  1.95it/s, loss=0.761]2025-01-12 12:28:04.838314: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Training: 100%|██████████| 3709/3709 [31:02<00:00,  1.99it/s, loss=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.7094\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3709/3709 [31:04<00:00,  1.99it/s, loss=0.765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6909\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3709/3709 [31:04<00:00,  1.99it/s, loss=0.675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6788\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3709/3709 [31:10<00:00,  1.98it/s, loss=0.733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6684\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3709/3709 [31:10<00:00,  2.01it/s, loss=0.664]2025-01-12 14:32:36.202929: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Training: 100%|██████████| 3709/3709 [31:10<00:00,  1.98it/s, loss=0.664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6607\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3709/3709 [31:09<00:00,  1.98it/s, loss=0.764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6537\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3709/3709 [31:13<00:00,  1.98it/s, loss=0.714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6481\n",
      "Generated caption: AA truck parked next to a truck parked next to a truck parked next to a truck parked on the side of a road.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "from transformers import TFGPT2Model, GPT2Tokenizer\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class ImageFeatureExtractor(layers.Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        super(ImageFeatureExtractor, self).__init__()\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "        base_model.trainable = False\n",
    "        self.cnn = base_model\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.projection = layers.Dense(output_dim, activation='relu')\n",
    "\n",
    "    def call(self, images):\n",
    "        x = self.cnn(images)\n",
    "        x = self.global_pool(x)\n",
    "        return self.projection(x)\n",
    "\n",
    "class ProjectionLayer(layers.Layer):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(ProjectionLayer, self).__init__()\n",
    "        self.dense = layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, image_features):\n",
    "        return self.dense(image_features)\n",
    "\n",
    "class ImageCaptioningModel(Model):\n",
    "    def __init__(self, max_length=50, vocab_size=50257):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.gpt2 = TFGPT2Model.from_pretrained('gpt2')\n",
    "        self.gpt2.trainable = False\n",
    "        self.image_encoder = ImageFeatureExtractor(\n",
    "            output_dim=self.gpt2.config.hidden_size\n",
    "        )\n",
    "        self.projection = ProjectionLayer(self.gpt2.config.hidden_size)\n",
    "        self.output_layer = layers.Dense(vocab_size, activation='softmax')\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def call(self, inputs):\n",
    "        images, text_tokens = inputs\n",
    "        image_features = self.image_encoder(images)\n",
    "        projected_features = self.projection(image_features)\n",
    "        text_outputs = self.gpt2(text_tokens, return_dict=True)\n",
    "        text_features = text_outputs.last_hidden_state\n",
    "        expanded_features = tf.expand_dims(projected_features, axis=1)\n",
    "        expanded_features = tf.tile(\n",
    "            expanded_features,\n",
    "            [1, tf.shape(text_features)[1], 1]\n",
    "        )\n",
    "        combined_features = text_features + expanded_features\n",
    "        outputs = self.output_layer(combined_features)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess a single image.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('RGB')\n",
    "    img = img.resize((224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
    "    return img_array\n",
    "\n",
    "def load_coco_annotations(annotations_file):\n",
    "    \"\"\"Load COCO dataset annotations.\"\"\"\n",
    "    print(f\"Loading annotations from {annotations_file}\")\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    image_captions = {}\n",
    "    for annotation in annotations['annotations']:\n",
    "        image_id = str(annotation['image_id']).zfill(12)\n",
    "        if image_id not in image_captions:\n",
    "            image_captions[image_id] = []\n",
    "        image_captions[image_id].append(annotation['caption'])\n",
    "\n",
    "    print(f\"Loaded annotations for {len(image_captions)} images\")\n",
    "    return image_captions\n",
    "\n",
    "\n",
    "def create_coco_dataset(image_dir, annotations_file, max_length, batch_size=16):\n",
    "    \"\"\"Create a dataset from COCO images and captions.\"\"\"\n",
    "    print(\"Loading COCO annotations...\")\n",
    "    image_captions = load_coco_annotations(annotations_file)\n",
    "    print(f\"Loaded {len(image_captions)} image captions.\")\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Adjust max_length to account for the shift between input and target\n",
    "    effective_length = max_length - 1\n",
    "\n",
    "    def generator():\n",
    "        image_ids = list(image_captions.keys())\n",
    "        while True:\n",
    "            random.shuffle(image_ids)\n",
    "            for image_id in image_ids:\n",
    "                image_path = os.path.join(image_dir, f\"{image_id}.jpg\")\n",
    "                if not os.path.exists(image_path):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    img = preprocess_image(image_path)\n",
    "                    caption = random.choice(image_captions[image_id])\n",
    "                    \n",
    "                    # Add special tokens and encode\n",
    "                    caption = f\"{tokenizer.bos_token}{caption}{tokenizer.eos_token}\"\n",
    "                    tokens = tokenizer.encode(\n",
    "                        caption,\n",
    "                        max_length=max_length,\n",
    "                        padding='max_length',\n",
    "                        truncation=True\n",
    "                    )\n",
    "                    \n",
    "                    # Create input and target sequences with consistent length\n",
    "                    input_tokens = tokens[:effective_length]  # First n-1 tokens\n",
    "                    target_tokens = tokens[1:effective_length+1]  # Shifted sequence\n",
    "                    \n",
    "                    # Pad sequences if necessary\n",
    "                    if len(input_tokens) < effective_length:\n",
    "                        padding = effective_length - len(input_tokens)\n",
    "                        input_tokens.extend([tokenizer.pad_token_id] * padding)\n",
    "                        target_tokens.extend([-100] * padding)\n",
    "                    \n",
    "                    # Convert to tensors\n",
    "                    input_tokens = tf.convert_to_tensor(input_tokens, dtype=tf.int32)\n",
    "                    target_tokens = tf.convert_to_tensor(target_tokens, dtype=tf.int32)\n",
    "                    \n",
    "                    yield (img, input_tokens), target_tokens\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    output_shapes = (\n",
    "        (\n",
    "            tf.TensorShape([224, 224, 3]),      # Image shape\n",
    "            tf.TensorShape([effective_length])   # Input tokens shape\n",
    "        ),\n",
    "        tf.TensorShape([effective_length])       # Target tokens shape\n",
    "    )\n",
    "\n",
    "    output_types = (\n",
    "        (tf.float32, tf.int32),  # Types for (image, input_tokens)\n",
    "        tf.int32                 # Type for target_tokens\n",
    "    )\n",
    "\n",
    "    # Create the dataset from the generator\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=output_types,\n",
    "        output_shapes=output_shapes\n",
    "    )\n",
    "\n",
    "    # Apply batching after the dataset is created\n",
    "    return dataset.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "def train_model(model, dataset, epochs=100, checkpoint_path='checkpoints/model'):\n",
    "    \"\"\"Train the model and save checkpoints.\"\"\"\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=False,\n",
    "        reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "    max_gradient_norm = 1.0\n",
    "    \n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "\n",
    "    steps_per_epoch = 3709\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(total=steps_per_epoch, desc=f\"Training\")\n",
    "        \n",
    "        for batch_inputs, batch_targets in dataset.take(steps_per_epoch):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_inputs)\n",
    "                \n",
    "                # Calculate loss with proper masking\n",
    "                mask = tf.not_equal(batch_targets, -100)\n",
    "                mask = tf.cast(mask, dtype=tf.float32)\n",
    "                \n",
    "                # Calculate per-token loss\n",
    "                token_losses = loss_fn(batch_targets, predictions)\n",
    "                \n",
    "                # Apply mask and calculate mean loss\n",
    "                masked_loss = token_losses * mask\n",
    "                loss = tf.reduce_sum(masked_loss) / tf.reduce_sum(mask)\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({'loss': float(loss)})\n",
    "            \n",
    "            if num_batches % 100 == 0:\n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "        progress_bar.close()\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        \n",
    "        model.save_weights(f\"{checkpoint_path}_epoch_from_train{epoch + 1}.weights.h5\")\n",
    "\n",
    "\n",
    "def generate_caption(model, image_path, max_length=50):\n",
    "    \"\"\"Generate a caption for a single image.\"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    img = preprocess_image(image_path)\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    current_tokens = tf.convert_to_tensor([[tokenizer.bos_token_id]], dtype=tf.int32)\n",
    "    generated_caption = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        predictions = model((img, current_tokens))\n",
    "        next_token = tf.argmax(predictions[:, -1, :], axis=-1)\n",
    "        token_id = int(next_token.numpy()[0])\n",
    "        generated_caption.append(token_id)\n",
    "\n",
    "        if token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        current_tokens = tf.convert_to_tensor([generated_caption], dtype=tf.int32)\n",
    "\n",
    "    caption = tokenizer.decode(generated_caption, skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set parameters\n",
    "    max_length = 50\n",
    "    epochs = 10\n",
    "    batch_size = 16\n",
    "    checkpoint_path = 'checkpoints/image_captioning_model'\n",
    "    print(\"set parameteres\")\n",
    "    # COCO dataset paths\n",
    "    image_dir = 'archive/coco2017/train2017'\n",
    "    annotations_file = 'archive/coco2017/annotations/captions_train2017.json'\n",
    "\n",
    "    # Create model and dataset\n",
    "    model = ImageCaptioningModel(max_length=max_length)\n",
    "    dataset = create_coco_dataset(\n",
    "        image_dir,\n",
    "        annotations_file,\n",
    "        max_length,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, dataset, epochs=epochs, checkpoint_path=checkpoint_path)\n",
    "\n",
    "    # Test the model (optional)\n",
    "    test_image_path = 'archive/coco2017/test2017/000000000001.jpg'  # Replace with your test image path\n",
    "    if os.path.exists(test_image_path):\n",
    "        generated_caption = generate_caption(model, test_image_path)\n",
    "        print(f\"Generated caption: {generated_caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: AA truck parked next to a truck parked next to a truck parked next to a truck parked on the side of a road.\n"
     ]
    }
   ],
   "source": [
    "# Test the model (optional)\n",
    "test_image_path = 'archive/coco2017/test2017/000000000001.jpg'  # Replace with your test image path\n",
    "if os.path.exists(test_image_path):\n",
    "    generated_caption = generate_caption(model, test_image_path)\n",
    "    print(f\"Generated caption: {generated_caption}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
